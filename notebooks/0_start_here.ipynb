{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "69bc8786-3ea7-4b21-a53f-09675d86534b",
      "metadata": {
        "collapsed": false,
        "name": "cell1"
      },
      "source": [
        "# Getting Started with pandas on Snowflake\n",
        "\n",
        "[pandas on Snowflake](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas) lets developers run their pandas code directly on their data in Snowflake. Users will be able to get the same pandas-native experience they know and love with Snowflake's performance, scale and governance.\n",
        "\n",
        "In this quickstart, we'll show how you can get started with running pandas on Snowflake through the Snowpark pandas API. We'll also see that the Snowpark pandas API is very similar to the native pandas API and enables you to scale up your traditional pandas pipelines with just a few lines of change. You can run this notebook in a Snowflake Notebook. \n",
        "\n",
        "## Using Snowpark pandas API\n",
        "\n",
        "The Snowpark pandas API is available as part of the Snowpark Python package (version 1.17 and above). Snowpark Python comes pre-installed with the Snowflake Notebooks environment. Additionally, you will need to add the `modin` package in the `Packages` dropdown.\n",
        "\n",
        "- To install Modin, select `modin` from `Packages` and ensure the version is 0.28.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b806a16b-b666-4e38-b11c-5db618772a12",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell2"
      },
      "outputs": [],
      "source": [
        "# Import the Snowpark pandas plugin for modin\n",
        "import snowflake.snowpark.modin.plugin\n",
        "import modin.pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e7b7455-c1ad-4ad8-bf85-1ed2b0d516b8",
      "metadata": {
        "collapsed": false,
        "name": "cell3"
      },
      "source": [
        "## Create Snowpark session\n",
        "Snowpark pandas requires an active `Session` object to connect to your data in Snowflake. In the next cell, we'll be initializing a Session object, and importing Snowpark pandas as `pd`. Make sure to use a database that you have write permissions on when creating the session, as Snowpark pandas requires write permissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2342c8-f661-4f86-8245-813f8b7ad0ab",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell4"
      },
      "outputs": [],
      "source": [
        "# Access current Snowpark session\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3812257-0a82-43a0-aaac-d00681558890",
      "metadata": {
        "collapsed": false,
        "name": "cell5"
      },
      "source": [
        "## Reading Data from Snowflake\n",
        "Today, we'll be analyzing the time series data from the [Cybersyn Finance and Economics dataset](https://app.snowflake.com/marketplace/listing/GZTSZAS2KF7/cybersyn-inc-financial-economic-essentials). You can find the instructions to setup the dataset for this tutorial [here](https://quickstarts.snowflake.com/guide/getting_started_with_pandas_on_snowflake/#1). \n",
        "\n",
        "Let's start by reading the `stock_price_timeseries` table into a DataFrame!\n",
        "\n",
        "Please double check that you have write permissions on the database that you initialized the Snowpark `Session` with. If you are reading from the `stock_price_timeseries` table, your `Session` needs to be configured to use a different database that you have write permissions on. The cell below uses the fully qualified name of the table to ensure that the read succeeds even though the `Session` is configured to use a different database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03298234-aabe-4548-99b1-bfdb609bdafb",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell6"
      },
      "outputs": [],
      "source": [
        "# Read data into a Snowpark pandas df \n",
        "from time import perf_counter\n",
        "start = perf_counter()\n",
        "spd_df = pd.read_snowflake(\"STOCK_PRICE_TIMESERIES\")\n",
        "end = perf_counter()\n",
        "data_size = len(spd_df)\n",
        "print(f\"Took {end - start} seconds to read a table with {data_size} rows into Snowpark pandas!\")\n",
        "snow_time = end - start"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f231df-fcc4-41e1-8809-314bae4b38ff",
      "metadata": {
        "collapsed": false,
        "name": "cell7"
      },
      "source": [
        "Now let's do the same by reading the data into vanilla pandas. There are two common approach to doing this: \n",
        "\n",
        "1) Create a [Snowpark DataFrame](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#return-the-contents-of-a-dataframe-as-a-pandas-dataframe) and calling [`to_pandas`](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.DataFrame.to_pandas) to export results into a pandas DataFrame\n",
        "```python\n",
        "snowpark_df = session.table(\"FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\n",
        "native_pd_df = snowpark_df.to_pandas()\n",
        "```\n",
        "\n",
        "2) Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-pandas) to query and export results from Snowflake into a pandas DataFrame using [`fetch_pandas_all`](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all)\n",
        "\n",
        "```python\n",
        "# Create a cursor object\n",
        "cur = session.connection.cursor()\n",
        "# Execute a statement that will generate a result set\n",
        "cur.execute(\"select * from FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\n",
        "# Fetch all the rows in a cursor and load them into a pandas DataFrame\n",
        "native_pd_df = cur.fetch_pandas_all()\n",
        "```\n",
        "\n",
        "We will use the second approach below and measure the time these operations take. (Note: This may take several minutes!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69ac2fd-c636-4bb5-a27d-58a5e4cbea7e",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "start = perf_counter()\n",
        "cur = session.connection.cursor()\n",
        "cur.execute(\"select * from FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\n",
        "native_pd_df = cur.fetch_pandas_all()\n",
        "end = perf_counter()\n",
        "print(f\"Native pandas took {end - start} seconds to read the data!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72085630-11c4-4df1-9627-7c50c0957906",
      "metadata": {
        "collapsed": false,
        "name": "cell9"
      },
      "source": [
        "As you can see, it takes much longer to export the Snowflake table into memory to operate with native pandas than for Snowpark pandas to read the table directly. This can also lead to the notebook session crashing if the exported data exceeds what can fit in memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c3dee5-62fe-46c6-a216-133a0140222d",
      "metadata": {
        "collapsed": false,
        "name": "cell10"
      },
      "source": [
        "## Examine The Raw Data\n",
        "Let's take a look at the data we're going to be working with. We will inspect the first five rows of the dataframe and print them out using Streamlit's interactive dataframe display (`st.dataframe`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a623bed-aed9-4cdb-a3c8-33e9e7da52af",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell11"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "st.dataframe(spd_df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2822e6ae-9810-4ca1-8646-660eb3e68d97",
      "metadata": {
        "collapsed": false,
        "name": "cell12"
      },
      "source": [
        "## Filtering The Data\n",
        "Let's take a look at some common data transformations - starting with filtering! Let's filter for stocks that are listed on the New York Stock Exchange!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4218fceb-68f1-41be-8c08-3f6ad51424d5",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell13"
      },
      "outputs": [],
      "source": [
        "start = perf_counter()\n",
        "nyse_spd_df = spd_df[(spd_df['PRIMARY_EXCHANGE_CODE'] == 'NYS')]\n",
        "repr(nyse_spd_df)\n",
        "end = perf_counter()\n",
        "st.dataframe(nyse_spd_df.head())\n",
        "print(f\"Filtering for stocks belonging to the NYSE took {end - start} seconds in Snowpark pandas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2e325b5-8e24-4dbc-93da-c2e93d84590f",
      "metadata": {
        "collapsed": false,
        "name": "cell14"
      },
      "source": [
        "Let's try an even more granular filter - let's filter for the Pre-Market Open of stocks that have the following tickers:\n",
        "* GOOG (Alphabet, Inc.)\n",
        "* MSFT (Microsoft)\n",
        "* SNOW (Snowflake)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d456c29-7689-4599-bcd6-02c646ef8f58",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell15"
      },
      "outputs": [],
      "source": [
        "start = perf_counter()\n",
        "filtered_spd_df = spd_df[((spd_df['TICKER'] == 'GOOG') | (spd_df['TICKER'] == 'MSFT') | (spd_df['TICKER'] == 'SNOW')) & (spd_df['VARIABLE_NAME'] == 'Pre-Market Open')]\n",
        "repr(filtered_spd_df)\n",
        "end = perf_counter()\n",
        "st.dataframe(filtered_spd_df.head())\n",
        "print(f\"Filtering for the Pre-Market Open price for the above stocks took {end - start} seconds in Snowpark pandas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d5cb07-ccce-481e-b41e-770d4de91b0f",
      "metadata": {
        "collapsed": false,
        "name": "cell16"
      },
      "source": [
        "# Reshaping the Data\n",
        "Let's say we wanted to analyse the performance of various stock prices across time - in that case, it may be more helpful to have the values as columns, and the ticker name and date as the index - rather than the current encoding. We can accomplish this using the `pivot_table` API!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8f893a-c7dc-4e08-bace-3c93ada282cf",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "name": "cell17"
      },
      "outputs": [],
      "source": [
        "start = perf_counter()\n",
        "reshape_df = spd_df.pivot_table(index=[\"TICKER\", \"DATE\"], columns=\"VARIABLE_NAME\", values=\"VALUE\")\n",
        "repr(reshape_df)\n",
        "end = perf_counter()\n",
        "print(f\"Pivoting the DataFrame took {end - start} seconds in Snowpark pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c0b9d1-a3be-4d05-9481-f54628f3b793",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell18"
      },
      "outputs": [],
      "source": [
        "st.dataframe(reshape_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3121c325-d7cf-47d6-a2d7-e759ece59d11",
      "metadata": {
        "collapsed": false,
        "name": "cell19"
      },
      "source": [
        "## Transforming the Data\n",
        "Now that we have reformatted the data, we can beginn to apply some transformations. Let's start by taking a look at the All-Day Low column for the tickers above - we can resample the data to look at the Quarterly Low for the `GOOG` ticker!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b06f23b-12dc-4387-bb87-bc4cbcff6a85",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "name": "cell20"
      },
      "outputs": [],
      "source": [
        "start = perf_counter()\n",
        "resampled_spd_df_all_quarter_low = reshape_df[\"All-Day Low\"][\"GOOG\"].resample(\"91D\").min()\n",
        "repr(resampled_spd_df_all_quarter_low)\n",
        "end = perf_counter()\n",
        "print(f\"Resampling the DataFrame took {end - start} seconds in Snowpark pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8978f55a-c28a-4b7f-9f20-4a2952d2a857",
      "metadata": {
        "collapsed": false,
        "language": "python",
        "name": "cell21"
      },
      "outputs": [],
      "source": [
        "print(resampled_spd_df_all_quarter_low)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c512e74d-7316-44de-a644-917270d38fac",
      "metadata": {
        "collapsed": false,
        "name": "cell22"
      },
      "source": [
        "We can even take a look at the quarter-over-quarter fluctuation in prices using the `diff` API!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb467dd6-cc74-423f-b17b-46541f5bbff8",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell23"
      },
      "outputs": [],
      "source": [
        "start = perf_counter()\n",
        "q_o_q_resampled_spd_df_all_quarter_low = resampled_spd_df_all_quarter_low.diff()\n",
        "repr(q_o_q_resampled_spd_df_all_quarter_low)\n",
        "end = perf_counter()\n",
        "print(f\"Diffing the resampled data took {end - start} seconds in Snowpark pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "866628d5-5bf9-4212-bba2-bf5e816a70e1",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell24"
      },
      "outputs": [],
      "source": [
        "print(q_o_q_resampled_spd_df_all_quarter_low)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7593697-feb5-40a7-9d6c-7c011ad35186",
      "metadata": {
        "collapsed": false,
        "name": "cell25"
      },
      "source": [
        "## Apply function along an axis\n",
        "Now we want to apply the absolute value square root on each value in the series. \n",
        "Snowpark pandas supports `apply`, which applies some arbitrary user-defined Python function along a particular axis of the DataFrame or Series. \n",
        "\n",
        "The Python function is serialized into Python bytecode and run this as a UDF inside Snowpark\u2019s Python secure sandbox runtime environment. Snowpark's Python runtime environment is seamlessly integrated with the Anaconda package manager so that users can leverage their favorite third-party packages such as NumPy for flexible data transformation within their dataframe apply. \n",
        "\n",
        "**Pro Tip:** While calling `apply` is convenient, since the underlying implementation are UDF or UDTFs, it may not be as optimized as SQL queries transpiled from other Snowpark pandas queries. If the function applied has an equivalent dataframe or series operation, we recommend using those operations instead. For example, instead of `df.groupby('col1').apply('sum')`, directly call `df.groupby('col1').sum()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3775af-8fe8-48f3-abd4-f783c0d27528",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell26"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "resampled_all_quarter_low_df_sqrt = q_o_q_resampled_spd_df_all_quarter_low.apply(\n",
        "    lambda x: np.sqrt(abs(x))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45dbe2f-8bd3-46db-8c31-f39994db6b99",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell27"
      },
      "outputs": [],
      "source": [
        "resampled_all_quarter_low_df_sqrt = resampled_all_quarter_low_df_sqrt.dropna()\n",
        "print(resampled_all_quarter_low_df_sqrt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a76af70b-c0f1-4884-9569-7187b3a16ff3",
      "metadata": {
        "collapsed": false,
        "name": "cell28"
      },
      "source": [
        "## Visualizing your results with Altair\n",
        "\n",
        "pandas is often used in conjunction with third-party visualization and machine learning libraries. Here we want to plot the quarter over quarter fluctuation in prices as a bar chart. \n",
        "\n",
        "\n",
        "First, let's clean up the data for plotting purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20345b5d-ef02-4641-9ed0-8cea05c5a6dd",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell29"
      },
      "outputs": [],
      "source": [
        "# Convert series to dataframe by resetting index\n",
        "plot_df = q_o_q_resampled_spd_df_all_quarter_low.reset_index()\n",
        "# Rename columns\n",
        "plot_df.columns = [\"DATE\", \"QLOW_DIFF\"]\n",
        "# Filter out extreme values\n",
        "plot_df = plot_df[plot_df[\"QLOW_DIFF\"]>-700]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8fc97f-c6fa-4393-a67b-bc751077dc00",
      "metadata": {
        "collapsed": false,
        "name": "cell30"
      },
      "source": [
        "When calling third-party library APIs with a Snowpark pandas dataframe, we recommend converting the Snowpark pandas dataframe to a pandas dataframe by calling [`to_pandas`](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.21.0/modin/pandas_api/snowflake.snowpark.modin.pandas.to_pandas) before passing the dataframe to the third-party library call. \n",
        "\n",
        "Note that calling to_pandas pulls your data out of Snowflake and into memory, so proceed with caution for large datasets and sensitive use cases. We generally recommend aggregating or summarizing and exporting only data that you will use for plotting using `to_pandas`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3d6647-86a4-4f32-9591-e8dfd4383c43",
      "metadata": {
        "codeCollapsed": false,
        "language": "python",
        "name": "cell31"
      },
      "outputs": [],
      "source": [
        "print(\"Input dataframe type: \", type(plot_df))\n",
        "pandas_plot_df = plot_df.to_pandas()\n",
        "print(\"After to_pandas, output dataframe type: \", type(pandas_plot_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd12f7cb-949f-462f-8806-79fe6b9734bb",
      "metadata": {
        "collapsed": false,
        "name": "cell32"
      },
      "source": [
        "Now we can use any Python visualization library, such as Altair, to plot the resulting pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb3a7d0-5a66-42a1-a887-b1748bf756fd",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell33"
      },
      "outputs": [],
      "source": [
        "import altair as alt\n",
        "alt.Chart(pandas_plot_df).mark_bar(width=10).encode(\n",
        "    x = alt.X(\"DATE:T\"),\n",
        "    y = alt.Y(\"QLOW_DIFF:Q\"),\n",
        "    color=alt.condition(\n",
        "        alt.datum.QLOW_DIFF > 0,\n",
        "        alt.value(\"steelblue\"),  # The positive color\n",
        "        alt.value(\"orange\")  # The negative color\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9bb36e-ec88-4ad6-8004-2968c548214d",
      "metadata": {
        "collapsed": false,
        "name": "cell34"
      },
      "source": [
        "### Conclusion\n",
        "pandas on Snowflake unlocks the power of Snowflake for pandas developers by allowing you to run the same pandas API, while operating on large data sets that don't typically work with native pandas and all while keeping your data in Snowflake! To learn more, see [Snowflake Documentation](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas). For a more advanced example, check out [this quickstart](https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_pandas/) on how you can build a data engineering pipeline with Snowpark pandas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}