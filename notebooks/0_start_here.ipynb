{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bc8786-3ea7-4b21-a53f-09675d86534b",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# Getting Started with pandas on Snowflake\n\n[pandas on Snowflake](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas) lets developers run their pandas code directly on their data in Snowflake. Users will be able to get the same pandas-native experience they know and love with Snowflake's performance, scale and governance.\n\nIn this quickstart, we'll show how you can get started with running pandas on Snowflake through the Snowpark pandas API. We'll also see that the Snowpark pandas API is very similar to the native pandas API and enables you to scale up your traditional pandas pipelines with just a few lines of change.\n\n## Using Snowpark pandas API\n\nThe Snowpark pandas API is available as part of the Snowpark Python package (version 1.17 and above). Snowpark Python comes pre-installed with the Snowflake Notebooks environment. Additionally, you will need to add the `modin` and `pandas` package in the `Packages` dropdown.\n\n- To install Modin, select `modin` from `Packages` and ensure the version is 0.28.1 or later.\n- To set the pandas version, select `pandas` from `Packages` and ensure the version is 2.2.1."
  },
  {
   "cell_type": "code",
   "id": "b806a16b-b666-4e38-b11c-5db618772a12",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Import the Snowpark pandas plugin for modin\nimport snowflake.snowpark.modin.plugin\nimport modin.pandas as pd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e7b7455-c1ad-4ad8-bf85-1ed2b0d516b8",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "## Create Snowpark session\nSnowpark pandas requires an active `Session` object to connect to your data in Snowflake. In the next cell, we'll be initializing a Session object, and importing Snowpark pandas as `pd`. Make sure to use a database that you have write permissions on when creating the session, as Snowpark pandas requires write permissions."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2342c8-f661-4f86-8245-813f8b7ad0ab",
   "metadata": {
    "name": "cell4",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Snowpark pandas has been in Public Preview since 1.17.0. See https://docs.snowflake.com/LIMITEDACCESS/snowpark-pandas for details.\n"
     ]
    }
   ],
   "source": "# Access current Snowpark session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()"
  },
  {
   "cell_type": "markdown",
   "id": "b3812257-0a82-43a0-aaac-d00681558890",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "## Reading Data from Snowflake\nToday, we'll be analyzing the time series data from the [Cybersyn Finance and Economics dataset](https://app.snowflake.com/marketplace/listing/GZTSZAS2KF7/cybersyn-inc-financial-economic-essentials) from [Snowflake Marketplace](https://www.snowflake.com/en/data-cloud/marketplace/). Let's start by reading the `stock_price_timeseries` table into a DataFrame!\n\nPlease double check that you have write permissions on the database that you initialized the Snowpark `Session` with. If you are reading from the `stock_price_timeseries` table, your `Session` needs to be configured to use a different database that you have write permissions on. The cell below uses the fully qualified name of the table to ensure that the read succeeds even though the `Session` is configured to use a different database."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03298234-aabe-4548-99b1-bfdb609bdafb",
   "metadata": {
    "name": "cell6",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 7.316147916999999 seconds to read a table with 66904580 rows into Snowpark pandas!\n"
     ]
    }
   ],
   "source": "# Read data into a Snowpark pandas df \nfrom time import perf_counter\nstart = perf_counter()\nspd_df = pd.read_snowflake(\"FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\nend = perf_counter()\ndata_size = len(spd_df)\nprint(f\"Took {end - start} seconds to read a table with {data_size} rows into Snowpark pandas!\")\nsnow_time = end - start"
  },
  {
   "cell_type": "markdown",
   "id": "28f231df-fcc4-41e1-8809-314bae4b38ff",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Now let's do the same by reading the data into vanilla pandas. There are two common approach to doing this: \n\n1) Create a [Snowpark DataFrame](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#return-the-contents-of-a-dataframe-as-a-pandas-dataframe) and calling [`to_pandas`](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.DataFrame.to_pandas) to export results into a pandas DataFrame\n```python\nsnowpark_df = session.table(\"FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\nnative_pd_df = snowpark_df.to_pandas()\n```\n\n2) Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-pandas) to query and export results from Snowflake into a pandas DataFrame using [`fetch_pandas_all`](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all)\n\n```python\n# Create a cursor object\ncur = session.connection.cursor()\n# Execute a statement that will generate a result set\ncur.execute(\"select * from FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\n# Fetch all the rows in a cursor and load them into a pandas DataFrame\nnative_pd_df = cur.fetch_pandas_all()\n```\n\nWe will use the second approach below and measure the time these operations take. (Note: This may take several minutes!)"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69ac2fd-c636-4bb5-a27d-58a5e4cbea7e",
   "metadata": {
    "scrolled": true,
    "name": "cell8",
    "language": "python",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native pandas took 475.227207208 seconds to read the data!\n"
     ]
    }
   ],
   "source": "start = perf_counter()\ncur = session.connection.cursor()\ncur.execute(\"select * from FINANCIAL__ECONOMIC_ESSENTIALS.CYBERSYN.STOCK_PRICE_TIMESERIES\")\nnative_pd_df = cur.fetch_pandas_all()\nend = perf_counter()\nprint(f\"Native pandas took {end - start} seconds to read the data!\")"
  },
  {
   "cell_type": "markdown",
   "id": "72085630-11c4-4df1-9627-7c50c0957906",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "As you can see, it takes much longer to export the Snowflake table into memory to operate with native pandas than for Snowpark pandas to read the table directly. This can also lead to the notebook session crashing if the exported data exceeds what can fit in memory."
  },
  {
   "cell_type": "markdown",
   "id": "62c3dee5-62fe-46c6-a216-133a0140222d",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "## Examine The Raw Data\nLet's take a look at the data we're going to be working with. We will inspect the first five rows of the dataframe and print them out using Streamlit's interactive dataframe display (`st.dataframe`)."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a623bed-aed9-4cdb-a3c8-33e9e7da52af",
   "metadata": {
    "name": "cell11",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TICKER</th>\n",
       "      <th>ASSET_CLASS</th>\n",
       "      <th>PRIMARY_EXCHANGE_CODE</th>\n",
       "      <th>PRIMARY_EXCHANGE_NAME</th>\n",
       "      <th>VARIABLE</th>\n",
       "      <th>VARIABLE_NAME</th>\n",
       "      <th>DATE</th>\n",
       "      <th>VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GDV</td>\n",
       "      <td>Closed-End Funds</td>\n",
       "      <td>NYS</td>\n",
       "      <td>NEW YORK STOCK EXCHANGE</td>\n",
       "      <td>pre-market_open</td>\n",
       "      <td>Pre-Market Open</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>22.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EELV</td>\n",
       "      <td>ETF-Index Fund Shares</td>\n",
       "      <td>PSE</td>\n",
       "      <td>NYSE ARCA</td>\n",
       "      <td>pre-market_open</td>\n",
       "      <td>Pre-Market Open</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>23.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCY</td>\n",
       "      <td>Equity</td>\n",
       "      <td>NYS</td>\n",
       "      <td>NEW YORK STOCK EXCHANGE</td>\n",
       "      <td>pre-market_open</td>\n",
       "      <td>Pre-Market Open</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>55.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISRL</td>\n",
       "      <td>Equity</td>\n",
       "      <td>NAS</td>\n",
       "      <td>NASDAQ CAPITAL MARKET</td>\n",
       "      <td>pre-market_open</td>\n",
       "      <td>Pre-Market Open</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>10.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACA</td>\n",
       "      <td>Equity</td>\n",
       "      <td>NYS</td>\n",
       "      <td>NEW YORK STOCK EXCHANGE</td>\n",
       "      <td>pre-market_open</td>\n",
       "      <td>Pre-Market Open</td>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>85.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TICKER            ASSET_CLASS PRIMARY_EXCHANGE_CODE  \\\n",
       "0    GDV       Closed-End Funds                   NYS   \n",
       "1   EELV  ETF-Index Fund Shares                   PSE   \n",
       "2    MCY                 Equity                   NYS   \n",
       "3   ISRL                 Equity                   NAS   \n",
       "4    ACA                 Equity                   NYS   \n",
       "\n",
       "     PRIMARY_EXCHANGE_NAME         VARIABLE    VARIABLE_NAME        DATE  \\\n",
       "0  NEW YORK STOCK EXCHANGE  pre-market_open  Pre-Market Open  2024-06-04   \n",
       "1                NYSE ARCA  pre-market_open  Pre-Market Open  2024-06-04   \n",
       "2  NEW YORK STOCK EXCHANGE  pre-market_open  Pre-Market Open  2024-06-04   \n",
       "3    NASDAQ CAPITAL MARKET  pre-market_open  Pre-Market Open  2024-06-04   \n",
       "4  NEW YORK STOCK EXCHANGE  pre-market_open  Pre-Market Open  2024-06-04   \n",
       "\n",
       "   VALUE  \n",
       "0  22.45  \n",
       "1  23.65  \n",
       "2  55.51  \n",
       "3  10.98  \n",
       "4  85.40  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "import streamlit as st\nst.dataframe(spd_df.head(5))"
  },
  {
   "cell_type": "markdown",
   "id": "2822e6ae-9810-4ca1-8646-660eb3e68d97",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": [
    "## Filtering The Data\n",
    "Let's take a look at some common data transformations - starting with filtering! Let's filter for stocks that are listed on the New York Stock Exchange!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4218fceb-68f1-41be-8c08-3f6ad51424d5",
   "metadata": {
    "name": "cell13",
    "language": "python",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for stocks belonging to the NYSE took 1.3051991250000015 seconds in Snowpark pandas\n"
     ]
    }
   ],
   "source": "start = perf_counter()\nnyse_spd_df = spd_df[(spd_df['PRIMARY_EXCHANGE_CODE'] == 'NYS')]\nrepr(nyse_spd_df)\nend = perf_counter()\nst.dataframe(nyse_spd_df.head())\nprint(f\"Filtering for stocks belonging to the NYSE took {end - start} seconds in Snowpark pandas\")"
  },
  {
   "cell_type": "markdown",
   "id": "c2e325b5-8e24-4dbc-93da-c2e93d84590f",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": [
    "Let's try an even more granular filter - let's filter for the Pre-Market Open of stocks that have the following tickers:\n",
    "* GOOG (Alphabet, Inc.)\n",
    "* MSFT (Microsoft)\n",
    "* SNOW (Snowflake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d456c29-7689-4599-bcd6-02c646ef8f58",
   "metadata": {
    "name": "cell15",
    "language": "python",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering for the Pre-Market Open price for the above stocks took 1.4652052499999968 seconds in Snowpark pandas\n"
     ]
    }
   ],
   "source": "start = perf_counter()\nfiltered_spd_df = spd_df[((spd_df['TICKER'] == 'GOOG') | (spd_df['TICKER'] == 'MSFT') | (spd_df['TICKER'] == 'SNOW')) & (spd_df['VARIABLE_NAME'] == 'Pre-Market Open')]\nrepr(filtered_spd_df)\nend = perf_counter()\nst.dataframe(filtered_spd_df.head())\nprint(f\"Filtering for the Pre-Market Open price for the above stocks took {end - start} seconds in Snowpark pandas\")"
  },
  {
   "cell_type": "markdown",
   "id": "a8d5cb07-ccce-481e-b41e-770d4de91b0f",
   "metadata": {
    "name": "cell16",
    "collapsed": false
   },
   "source": [
    "# Reshaping the Data\n",
    "Let's say we wanted to analyse the performance of various stock prices across time - in that case, it may be more helpful to have the values as columns, and the ticker name and date as the index - rather than the current encoding. We can accomplish this using the `pivot_table` API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8f893a-c7dc-4e08-bace-3c93ada282cf",
   "metadata": {
    "name": "cell17",
    "language": "python",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivoting the DataFrame took 4.764952458999971 seconds in Snowpark pandas\n"
     ]
    }
   ],
   "source": "start = perf_counter()\nreshape_df = spd_df.pivot_table(index=[\"TICKER\", \"DATE\"], columns=\"VARIABLE_NAME\", values=\"VALUE\")\nrepr(reshape_df)\nend = perf_counter()\nprint(f\"Pivoting the DataFrame took {end - start} seconds in Snowpark pandas\")"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c0b9d1-a3be-4d05-9481-f54628f3b793",
   "metadata": {
    "name": "cell18",
    "language": "python",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VARIABLE_NAME</th>\n",
       "      <th>All-Day High</th>\n",
       "      <th>All-Day Low</th>\n",
       "      <th>Nasdaq Volume</th>\n",
       "      <th>Post-Market Close</th>\n",
       "      <th>Pre-Market Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TICKER</th>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>66.35</td>\n",
       "      <td>65.50</td>\n",
       "      <td>439231.0</td>\n",
       "      <td>66.23</td>\n",
       "      <td>65.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-02</th>\n",
       "      <td>66.86</td>\n",
       "      <td>65.81</td>\n",
       "      <td>316586.0</td>\n",
       "      <td>65.91</td>\n",
       "      <td>66.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-03</th>\n",
       "      <td>66.46</td>\n",
       "      <td>64.85</td>\n",
       "      <td>407491.0</td>\n",
       "      <td>66.33</td>\n",
       "      <td>65.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-04</th>\n",
       "      <td>67.25</td>\n",
       "      <td>65.63</td>\n",
       "      <td>269025.0</td>\n",
       "      <td>66.99</td>\n",
       "      <td>66.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-07</th>\n",
       "      <td>67.98</td>\n",
       "      <td>67.08</td>\n",
       "      <td>263454.0</td>\n",
       "      <td>67.40</td>\n",
       "      <td>67.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "VARIABLE_NAME      All-Day High  All-Day Low  Nasdaq Volume  \\\n",
       "TICKER DATE                                                   \n",
       "A      2018-05-01         66.35        65.50       439231.0   \n",
       "       2018-05-02         66.86        65.81       316586.0   \n",
       "       2018-05-03         66.46        64.85       407491.0   \n",
       "       2018-05-04         67.25        65.63       269025.0   \n",
       "       2018-05-07         67.98        67.08       263454.0   \n",
       "\n",
       "VARIABLE_NAME      Post-Market Close  Pre-Market Open  \n",
       "TICKER DATE                                            \n",
       "A      2018-05-01              66.23            65.64  \n",
       "       2018-05-02              65.91            66.01  \n",
       "       2018-05-03              66.33            65.91  \n",
       "       2018-05-04              66.99            66.06  \n",
       "       2018-05-07              67.40            67.16  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "st.dataframe(reshape_df.head())"
  },
  {
   "cell_type": "markdown",
   "id": "3121c325-d7cf-47d6-a2d7-e759ece59d11",
   "metadata": {
    "name": "cell19",
    "collapsed": false
   },
   "source": [
    "## Transforming the Data\n",
    "Now that we have reformatted the data, we can beginn to apply some transformations. Let's start by taking a look at the All-Day Low column for the tickers above - we can resample the data to look at the Quarterly Low for the `GOOG` ticker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b06f23b-12dc-4387-bb87-bc4cbcff6a85",
   "metadata": {
    "name": "cell20",
    "language": "python",
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling the DataFrame took 2.1228156670000544 seconds in Snowpark pandas\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "resampled_spd_df_all_quarter_low = reshape_df[\"All-Day Low\"][\"GOOG\"].resample(\"91D\").min()\n",
    "repr(resampled_spd_df_all_quarter_low)\n",
    "end = perf_counter()\n",
    "print(f\"Resampling the DataFrame took {end - start} seconds in Snowpark pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8978f55a-c28a-4b7f-9f20-4a2952d2a857",
   "metadata": {
    "name": "cell21",
    "language": "python",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "2018-05-01    1006.48\n",
       "2018-07-31     995.58\n",
       "2018-10-30     968.09\n",
       "2019-01-29    1055.85\n",
       "2019-04-30    1025.06\n",
       "2019-07-30    1125.00\n",
       "2019-10-29    1250.79\n",
       "2020-01-28    1013.54\n",
       "2020-04-28    1218.04\n",
       "2020-07-28    1399.96\n",
       "2020-10-27    1514.61\n",
       "2021-01-26    1801.22\n",
       "2021-04-27    2223.89\n",
       "2021-07-27    2623.00\n",
       "2021-10-26    2493.01\n",
       "2022-01-25    2363.60\n",
       "2022-04-26     107.01\n",
       "2022-07-26      94.93\n",
       "2022-10-25      83.00\n",
       "2023-01-24      88.87\n",
       "2023-04-25     101.66\n",
       "2023-07-25     121.54\n",
       "2023-10-24     121.47\n",
       "2024-01-23     131.54\n",
       "2024-04-23     152.77\n",
       "Freq: None, Name: All-Day Low, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "print(resampled_spd_df_all_quarter_low)"
  },
  {
   "cell_type": "markdown",
   "id": "c512e74d-7316-44de-a644-917270d38fac",
   "metadata": {
    "name": "cell22",
    "collapsed": false
   },
   "source": [
    "We can even take a look at the quarter-over-quarter fluctuation in prices using the `diff` API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb467dd6-cc74-423f-b17b-46541f5bbff8",
   "metadata": {
    "name": "cell23",
    "language": "python",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diffing the resampled data took 1.2249565410000969 seconds in Snowpark pandas\n"
     ]
    }
   ],
   "source": [
    "start = perf_counter()\n",
    "q_o_q_resampled_spd_df_all_quarter_low = resampled_spd_df_all_quarter_low.diff()\n",
    "repr(q_o_q_resampled_spd_df_all_quarter_low)\n",
    "end = perf_counter()\n",
    "print(f\"Diffing the resampled data took {end - start} seconds in Snowpark pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "866628d5-5bf9-4212-bba2-bf5e816a70e1",
   "metadata": {
    "name": "cell24",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "2018-05-01        NaN\n",
       "2018-07-31     -10.90\n",
       "2018-10-30     -27.49\n",
       "2019-01-29      87.76\n",
       "2019-04-30     -30.79\n",
       "2019-07-30      99.94\n",
       "2019-10-29     125.79\n",
       "2020-01-28    -237.25\n",
       "2020-04-28     204.50\n",
       "2020-07-28     181.92\n",
       "2020-10-27     114.65\n",
       "2021-01-26     286.61\n",
       "2021-04-27     422.67\n",
       "2021-07-27     399.11\n",
       "2021-10-26    -129.99\n",
       "2022-01-25    -129.41\n",
       "2022-04-26   -2256.59\n",
       "2022-07-26     -12.08\n",
       "2022-10-25     -11.93\n",
       "2023-01-24       5.87\n",
       "2023-04-25      12.79\n",
       "2023-07-25      19.88\n",
       "2023-10-24      -0.07\n",
       "2024-01-23      10.07\n",
       "2024-04-23      21.23\n",
       "Freq: None, Name: All-Day Low, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "print(q_o_q_resampled_spd_df_all_quarter_low)"
  },
  {
   "cell_type": "markdown",
   "id": "a7593697-feb5-40a7-9d6c-7c011ad35186",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "## Apply function along an axis\nNow we want to apply the absolute value square root on each value in the series. \nSnowpark pandas supports `apply`, which applies some arbitrary user-defined Python function along a particular axis of the DataFrame or Series. \n\nThe Python function is serialized into Python bytecode and run this as a UDF inside Snowpark’s Python secure sandbox runtime environment. Snowpark's Python runtime environment is seamlessly integrated with the Anaconda package manager so that users can leverage their favorite third-party packages such as NumPy for flexible data transformation within their dataframe apply. \n\n**Pro Tip:** While calling `apply` is convenient, since the underlying implementation are UDF or UDTFs, it may not be as optimized as SQL queries transpiled from other Snowpark pandas queries. If the function applied has an equivalent dataframe or series operation, we recommend using those operations instead. For example, instead of `df.groupby('col1').apply('sum')`, directly call `df.groupby('col1').sum()`."
  },
  {
   "cell_type": "code",
   "id": "0c3775af-8fe8-48f3-abd4-f783c0d27528",
   "metadata": {
    "language": "python",
    "name": "cell27",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import numpy as np\nresampled_all_quarter_low_df_sqrt = q_o_q_resampled_spd_df_all_quarter_low.apply(\n    lambda x: np.sqrt(abs(x))\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e45dbe2f-8bd3-46db-8c31-f39994db6b99",
   "metadata": {
    "language": "python",
    "name": "cell28",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "resampled_all_quarter_low_df_sqrt = resampled_all_quarter_low_df_sqrt.dropna()\nprint(resampled_all_quarter_low_df_sqrt)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a76af70b-c0f1-4884-9569-7187b3a16ff3",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "## Visualizing your results with Altair\n\npandas is often used in conjunction with third-party visualization and machine learning libraries. Here we want to plot the quarter over quarter fluctuation in prices as a bar chart. \n\n\nFirst, let's clean up the data for plotting purposes."
  },
  {
   "cell_type": "code",
   "id": "20345b5d-ef02-4641-9ed0-8cea05c5a6dd",
   "metadata": {
    "language": "python",
    "name": "cell33",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Convert series to dataframe by resetting index\nplot_df = q_o_q_resampled_spd_df_all_quarter_low.reset_index()\n# Rename columns\nplot_df.columns = [\"DATE\", \"QLOW_DIFF\"]\n# Filter out extreme values\nplot_df = plot_df[plot_df[\"QLOW_DIFF\"]>-700]",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a8fc97f-c6fa-4393-a67b-bc751077dc00",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "When calling third-party library APIs with a Snowpark pandas dataframe, we recommend converting the Snowpark pandas dataframe to a pandas dataframe by calling [`to_pandas`](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/1.21.0/modin/pandas_api/snowflake.snowpark.modin.pandas.to_pandas) before passing the dataframe to the third-party library call. \n\nNote that calling to_pandas pulls your data out of Snowflake and into memory, so proceed with caution for large datasets and sensitive use cases. We generally recommend aggregating or summarizing and exporting only data that you will use for plotting using `to_pandas`.\n"
  },
  {
   "cell_type": "code",
   "id": "5c3d6647-86a4-4f32-9591-e8dfd4383c43",
   "metadata": {
    "language": "python",
    "name": "cell26",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "print(\"Input dataframe type: \", type(plot_df))\npandas_plot_df = plot_df.to_pandas()\nprint(\"After to_pandas, output dataframe type: \", type(pandas_plot_df))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd12f7cb-949f-462f-8806-79fe6b9734bb",
   "metadata": {
    "name": "cell34",
    "collapsed": false
   },
   "source": "Now we can use any Python visualization library, such as Altair, to plot the resulting pandas dataframe."
  },
  {
   "cell_type": "code",
   "id": "ceb3a7d0-5a66-42a1-a887-b1748bf756fd",
   "metadata": {
    "language": "python",
    "name": "cell31",
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "import altair as alt\nalt.Chart(pandas_plot_df).mark_bar(width=10).encode(\n    x = alt.X(\"DATE:T\"),\n    y = alt.Y(\"QLOW_DIFF:Q\"),\n    color=alt.condition(\n        alt.datum.QLOW_DIFF > 0,\n        alt.value(\"steelblue\"),  # The positive color\n        alt.value(\"orange\")  # The negative color\n    )\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c9bb36e-ec88-4ad6-8004-2968c548214d",
   "metadata": {
    "name": "cell29",
    "collapsed": false
   },
   "source": "### Conclusion\npandas on Snowflake unlocks the power of Snowflake for pandas developers by allowing you to run the same pandas API, while operating on large data sets that don't typically work with native pandas and all while keeping your data in Snowflake! To learn more, see [Snowflake Documentation](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas). For a more advanced example, check out [this quickstart](https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_pandas/) on how you can build a data engineering pipeline with Snowpark pandas."
  }
 ]
}